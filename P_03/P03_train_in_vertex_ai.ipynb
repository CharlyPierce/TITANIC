{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenaremos un modelo en Vertex AI haciendo uso de su infraestructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0QgF-8o4Ctwo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero definiremos un modelo encargado de entrenar datos ficticios\n",
    "#### El propósito es la implementación del entrenamiento en la nube\n",
    "Para definir el modelo, podemos usar alguna de las tres formas de construir modelos en TensorFlow:\n",
    "* Modelo secuencial\n",
    "* Modelo funcional\n",
    "* Modelo por subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yA4UKnYGCyf0"
   },
   "outputs": [],
   "source": [
    "# Modelo Funcional\n",
    "x0 = tf.keras.Input(shape=(10,))  \n",
    "x = tf.keras.layers.Dense(10)(x0)\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "model_functional = tf.keras.Model(x0, x)  \n",
    "\n",
    "# Modelo Secuencial\n",
    "model_sequential = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "# Subclassing\n",
    "class MyModel(tf.keras.Model):  # Hereda de tf.keras.Model.\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "class MyModel2(MyModel):  # Hereda de MyModel.\n",
    "    def __init__(self):\n",
    "        super(MyModel2, self).__init__()\n",
    "        self.dense2 = tf.keras.layers.Dense(10)\n",
    "        self.dense3 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = super().call(x)  # Llama al método call de MyModel.\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rV6quGzMt3fk"
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convierte y_true a float32\n",
    "    error = y_pred - y_true\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "strategy = tf.distribute.get_strategy()  # Use the Default Strategy\n",
    "#strategy = tf.distribute.MirroredStrategy() # varias GPUs en una máquina.\n",
    "# strategy = tf.distribute.MultiWorkerMirroredStrategy() # múltiples máquinas, cada una con una o varias GPUs.\n",
    "# Abrir un scope de estrategia\n",
    "with strategy.scope():\n",
    "    # Todo lo que crees dentro de este bloque será distribuido\n",
    "    model = MyModel2()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=custom_loss,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- También crearemos un ModelCheckpoint para guardar el modelo, ya sea en local o en la nube. Le diremos que solo guarde el mejor, basado en una métrica.\n",
    "\n",
    "- Probaremos el código para asegurarnos de que funcionará en la nube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXlRTUqEwfDx",
    "outputId": "6adcf89c-0e6b-4b22-b416-769bbe88b951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/32 [..............................] - ETA: 11s - loss: 2.4936 - accuracy: 0.4688\n",
      "Epoch 1: accuracy improved from -inf to 0.46900, saving model to model/m0\n",
      "INFO:tensorflow:Assets written to: model/m0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/m0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 9ms/step - loss: 1.2195 - accuracy: 0.4690\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.8309 - accuracy: 0.5150\n",
      "[0.8308799266815186, 0.5149999856948853]\n",
      "INFO:tensorflow:Assets written to: model/m1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/m1/assets\n"
     ]
    }
   ],
   "source": [
    "# Crear un callback de ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('model/m0',\n",
    "                             monitor='accuracy',  # la métrica a monitorizar\n",
    "                             verbose=1,  # log level\n",
    "                             save_best_only=True,  # solo guarda el mejor modelo\n",
    "                             mode='max',  # modo 'max' porque queremos maximizar la accuracy\n",
    "                             save_format='tf')  # especifica que se debe usar el formato SavedModel\n",
    "\n",
    "x_train = np.random.random((1000, 10))\n",
    "y_train = np.random.randint(2, size=(1000,))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=1,\n",
    "          batch_size=32,\n",
    "          callbacks=[checkpoint])  # asegúrate de pasar el callback aquí\n",
    "\n",
    "# Evaluar el modelo\n",
    "x_test = np.random.random((200, 10))\n",
    "y_test = np.random.randint(2, size=(200,))\n",
    "evv = model.evaluate(x_test, y_test)\n",
    "print(evv)\n",
    "model.save('model/m1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos una carpeta que contendrá los archivos para nuestra imagen Docker. \n",
    "\n",
    "- Estos constan de un Dockerfile, requirements.txt y el script train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WosPhUoCFZuy"
   },
   "outputs": [],
   "source": [
    "!mkdir -p train_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcggkDaGyml7",
    "outputId": "48efca18-2c9c-4a5b-dcdf-ad8b81fee16e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_dir/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_dir/train.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Modelo Secuencial\n",
    "model_sequential = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "strategy = tf.distribute.get_strategy() \n",
    "#strategy = tf.distribute.MirroredStrategy()  # varias GPUs en una máquina.\n",
    "# Abrir un scope de estrategia\n",
    "with strategy.scope():\n",
    "    model = model_sequential\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Generar algunos datos de ejemplo\n",
    "x_train = np.random.random((1000, 10))\n",
    "y_train = np.random.randint(2, size=(1000,))\n",
    "\n",
    "# Crear un callback de ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('gs://models_ai_save/best_model',\n",
    "                             monitor='accuracy',  # la métrica a monitorizar\n",
    "                             verbose=1,  # log level\n",
    "                             save_best_only=True,  # solo guarda el mejor modelo\n",
    "                             mode='max',  # modo 'max' porque queremos maximizar la accuracy\n",
    "                             save_format='tf')  # especifica que se debe usar el formato SavedModel\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=32, callbacks=[checkpoint])\n",
    "\n",
    "# Evaluar el modelo\n",
    "x_test = np.random.random((200, 10))\n",
    "y_test = np.random.randint(2, size=(200,))\n",
    "evv = model.evaluate(x_test, y_test)\n",
    "print(evv)\n",
    "\n",
    "# Guardar el modelo final en el bucket de GCS\n",
    "model.save('gs://models_ai_save/model_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mR4WnNG1rJ6H",
    "outputId": "a2978a38-bbb4-4c9f-f81e-e23820cedc8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_dir/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_dir/Dockerfile\n",
    "\n",
    "# Usa la imagen base de TensorFlow con soporte para GPU\n",
    "FROM python:3.9.2\n",
    "\n",
    "# Actualiza pip\n",
    "RUN pip install --upgrade pip\n",
    "\n",
    "# Copia el archivo requirements.txt al contenedor\n",
    "COPY requirements.txt /requirements.txt\n",
    "\n",
    "# Instala las dependencias\n",
    "RUN pip install -r /requirements.txt\n",
    "\n",
    "# Copia el script de entrenamiento al contenedor\n",
    "COPY train.py /train.py\n",
    "\n",
    "# Establece el script de entrenamiento como el comando predeterminado del contenedor\n",
    "CMD [\"python\", \"/train.py\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "JsessTuXKoNp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_dir/.dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_dir/.dockerignore\n",
    "\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.egg-info\n",
    "**/model/m0/\n",
    "**/model/m1/\n",
    "\n",
    "env/\n",
    ".git\n",
    ".dockerignore\n",
    ".gitignore\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.egg-info\n",
    "*.egg\n",
    "\n",
    "# Entornos virtuales\n",
    "venv/\n",
    ".env/\n",
    "env\n",
    "**/__pycache__\n",
    "__pycache__\n",
    "*.virtualenv/\n",
    "\n",
    "# Generales\n",
    ".DS_Store\n",
    "*.log\n",
    ".idea/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Una vez con los archivos en la carpeta, desde la terminal deberemos construir la imagen Docker, etiquetarla y subirla a nuestro repositorio en Artifact Registry. \n",
    "\n",
    "- Esto se debe hacer en una terminal fuera de Jupyter Notebook, ya que Jupyter no permite la ejecución de Docker sobre él."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecutamos en terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### docker build -t train_model .\n",
    "### docker tag train_model us-central1-docker.pkg.dev/projecto2-373519/myimages/train_model:v1.0.0\n",
    "### docker push us-central1-docker.pkg.dev/projecto2-373519/myimages/train_model:v1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Una vez con la imagen en la nube, ejecutamos el siguiente comando para crear un custom job o entrenamiento personalizado. \n",
    "* En este caso, le damos una ubicación, un nombre, el tipo de máquina y las réplicas, mayor a 1 en caso de usar estrategias distribuidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/936401695274/locations/us-central1/customJobs/3065650967581032448] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/936401695274/locations/us-central1/customJobs/3065650967581032448\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/936401695274/locations/us-central1/customJobs/3065650967581032448\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs create \\\n",
    "  --region=us-central1 \\\n",
    "  --display-name=entrenamiento_prueba_2 \\\n",
    "  --worker-pool-spec=machine-type=n1-standard-4,replica-count=1,container-image-uri=us-central1-docker.pkg.dev/projecto2-373519/myimages/train_model_cpu:v1.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2023-09-28T08:40:55.110080Z'\n",
      "displayName: entrenamiento_prueba_2\n",
      "jobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      imageUri: us-central1-docker.pkg.dev/projecto2-373519/myimages/train_model_cpu:v1.0.0\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-4\n",
      "    replicaCount: '1'\n",
      "name: projects/936401695274/locations/us-central1/customJobs/3065650967581032448\n",
      "startTime: '2023-09-28T08:40:55.261758Z'\n",
      "state: JOB_STATE_PENDING\n",
      "updateTime: '2023-09-28T08:41:15.974667Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs describe projects/936401695274/locations/us-central1/customJobs/3065650967581032448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoreo del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "INFO\t2023-09-28 02:40:55 -0600\tservice\tWaiting for job to be provisioned.\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\t2023-09-28 08:42:08.856615: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\t2023-09-28 08:42:08.915167: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\t2023-09-28 08:42:08.915250: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\t2023-09-28 08:42:08.915299: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\t2023-09-28 08:42:08.925572: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\t2023-09-28 08:42:08.925947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "ERROR\t2023-09-28 02:42:08 -0600\tworkerpool0-0\tTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ERROR\t2023-09-28 02:42:10 -0600\tworkerpool0-0\t2023-09-28 08:42:10.331714: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "INFO\t2023-09-28 02:42:11 -0600\tworkerpool0-0\tEpoch 1/3\n",
      "INFO\t2023-09-28 02:42:12 -0600\tworkerpool0-0\t\n",
      "INFO\t2023-09-28 02:42:12 -0600\tworkerpool0-0\t 1/32 [..............................] - ETA: 25s - loss: 8.6765 - accuracy: 0.437\n",
      "INFO\t2023-09-28 02:42:12 -0600\tworkerpool0-0\t26/32 [=======================>......] - ETA: 0s - loss: 7.8979 - accuracy: 0.4880 \n",
      "INFO\t2023-09-28 02:42:12 -0600\tworkerpool0-0\tEpoch 1: accuracy improved from -inf to 0.49400, saving model to gs://models_ai_save/best_model\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t32/32 [==============================] - 12s 364ms/step - loss: 7.8050 - accuracy: 0.4940\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\tEpoch 2/3\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t 1/32 [..............................] - ETA: 0s - loss: 10.1226 - accuracy: 0.343\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t28/32 [=========================>....] - ETA: 0s - loss: 7.9018 - accuracy: 0.4877 \n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\tEpoch 2: accuracy did not improve from 0.49400\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t32/32 [==============================] - 0s 2ms/step - loss: 7.8050 - accuracy: 0.4940\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\tEpoch 3/3\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t 1/32 [..............................] - ETA: 0s - loss: 9.6406 - accuracy: 0.37\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t31/32 [============================>.] - ETA: 0s - loss: 7.7902 - accuracy: 0.4950\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\tEpoch 3: accuracy did not improve from 0.49400\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t32/32 [==============================] - 0s 2ms/step - loss: 7.8050 - accuracy: 0.4940\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t1/7 [===>..........................] - ETA: 1s - loss: 7.7125 - accuracy: 0.50\n",
      "INFO\t2023-09-28 02:42:23 -0600\tworkerpool0-0\t7/7 [==============================] - 0s 2ms/step - loss: 7.9438 - accuracy: 0.4850\n",
      "INFO\t2023-09-28 02:42:33 -0600\tworkerpool0-0\t[7.943849086761475, 0.48500001430511475]\n",
      "INFO\t2023-09-28 02:47:20 -0600\tservice\tJob completed successfully.\n",
      "^C\n",
      "\n",
      "\n",
      "Command killed by keyboard interrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs stream-logs projects/936401695274/locations/us-central1/customJobs/3065650967581032448"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
